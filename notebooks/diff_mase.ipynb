{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('venv': virtualenv)",
   "display_name": "Python 3.6.9 64-bit ('venv': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "372b0b7ec2deba16ae067aa3df2758e57c96731335b7f57a0616055622e60b0e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M4Dataset(Dataset):\n",
    "    \"\"\" Dataset for M4 AR(P) models \"\"\"\n",
    "\n",
    "    def __init__(self, arr, debug=False):\n",
    "        if debug:\n",
    "            np.random.shuffle(arr)\n",
    "            self.X = torch.from_numpy(arr[:100, :])\n",
    "        else:\n",
    "            self.X = torch.from_numpy(arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :-1], self.X[idx, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, mem=2, n_hidden=8):\n",
    "        super(MLP, self).__init__()\n",
    "        self.h1 = nn.Linear(mem, n_hidden)\n",
    "        self.h2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h4 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h5 = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h1(x))\n",
    "        x = F.relu(self.h2(x))\n",
    "        x = F.relu(self.h3(x))\n",
    "        x = F.relu(self.h4(x))\n",
    "        x = self.h5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    N layer multi layer perceptron with H hidden units in each layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features:int, out_features:int=1, num_layers:int=5, n_hidden:int=32, bias:bool=True):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.N = num_layers\n",
    "        self.H = n_hidden\n",
    "        self.memory = in_features\n",
    "        self.bias=bias\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            self.layers.append(nn.Linear(in_features=in_features if i==0 else n_hidden, out_features=n_hidden, bias=bias))\n",
    "        self.out = nn.Linear(in_features=n_hidden, out_features=out_features, bias=bias)\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.N-1):\n",
    "            x = F.relu(self.layers[i](x))\n",
    "        out = self.out(x)\n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for i in range(self.N - 1):\n",
    "            nn.init.normal_(self.layers[i].weight, std=1e-3)\n",
    "            if self.bias:\n",
    "                nn.init.normal_(self.layers[i].bias, std=1e-6)\n",
    "        nn.init.normal_(self.out.weight, std=1e-3)\n",
    "        if self.bias:\n",
    "            nn.init.normal_(self.out.bias, std=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n",
      "3617 trainable parameters in the network.\n",
      "Epoch 10 train loss: 61.938711143443065\n",
      "Epoch 10 val loss  : 36.07511596692862\n",
      "Early stop count = 0\n",
      "Epoch 20 train loss: 58.1963314347814\n",
      "Epoch 20 val loss  : 32.775944275808136\n",
      "Early stop count = 0\n",
      "Epoch 30 train loss: 56.31120307867703\n",
      "Epoch 30 val loss  : 31.903761648488384\n",
      "Early stop count = 6\n",
      "Epoch 40 train loss: 55.7859607509825\n",
      "Epoch 40 val loss  : 32.15770046339768\n",
      "Early stop count = 4\n",
      "Epoch 50 train loss: 57.19517395771521\n",
      "Epoch 50 val loss  : 31.068936054665862\n",
      "Early stop count = 5\n",
      "Epoch 60 train loss: 53.75267256008897\n",
      "Epoch 60 val loss  : 30.94843137158065\n",
      "Early stop count = 2\n",
      "Epoch 70 train loss: 51.28433543821948\n",
      "Epoch 70 val loss  : 29.90660285066655\n",
      "Early stop count = 2\n",
      "Epoch 80 train loss: 51.68307633564861\n",
      "Epoch 80 val loss  : 30.701617974942586\n",
      "Early stop count = 5\n",
      "Epoch 90 train loss: 50.8129419784539\n",
      "Epoch 90 val loss  : 29.754989374230032\n",
      "Early stop count = 0\n",
      "Epoch 100 train loss: 51.3415619067958\n",
      "Epoch 100 val loss  : 29.468369884752203\n",
      "Early stop count = 9\n",
      "Epoch 110 train loss: 52.37529210368308\n",
      "Epoch 110 val loss  : 29.986488628291077\n",
      "Early stop count = 5\n",
      "Epoch 120 train loss: 51.35969893969473\n",
      "Epoch 120 val loss  : 30.285461499852712\n",
      "Early stop count = 3\n",
      "Epoch 130 train loss: 49.59099228656863\n",
      "Epoch 130 val loss  : 29.568344420835217\n",
      "Early stop count = 13\n",
      "Epoch 140 train loss: 50.67123995547787\n",
      "Epoch 140 val loss  : 30.10919637576332\n",
      "Early stop count = 23\n",
      "Early stop after epoch 143.\n",
      "Finished training!\n",
      "MASE : 0.8634252602595608\n",
      "MASE axis : 0.8634252602595606\n",
      "MASE yearly: 1.4482560335109298\n",
      "MASE axis yearly : 1.4482560335109296\n",
      "MASE recursive yearly : 3.384445465082836\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(1729)\n",
    "#torch.random.manual_seed(1729)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "debug = False\n",
    "\n",
    "X_train = np.load(\"../M4_GLOBAL_train.npy\")\n",
    "X_test = np.load(\"../M4_GLOBAL_test.npy\")\n",
    "\n",
    "train = X_train[: int(X_train.shape[0] * 0.8), :]\n",
    "val = X_train[int(X_train.shape[0] * 0.8) :, :]\n",
    "\n",
    "#model = MLP(mem=12, n_hidden=32).double()\n",
    "model = MLP2(in_features=12, out_features=1, num_layers=5, n_hidden=32).double()\n",
    "model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{pytorch_total_params} trainable parameters in the network.\")\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "ds = M4Dataset(arr=train, debug=False)\n",
    "ds_val = M4Dataset(arr=val, debug=False)\n",
    "trainloader = DataLoader(dataset=ds, batch_size=1024, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(dataset=ds_val, batch_size=1024, shuffle=True, num_workers=0)\n",
    "\n",
    "num_epochs = 1000\n",
    "tenacity = 25\n",
    "early_stop_count = 0\n",
    "low_loss = np.inf\n",
    "np.random.seed(1729)\n",
    "torch.manual_seed(1729)\n",
    "for ep in range(1, num_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.flatten(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.flatten(), labels)\n",
    "            val_loss += loss.item()\n",
    "        if val_loss < low_loss:\n",
    "            early_stop_count = 0\n",
    "            low_loss = val_loss\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        if early_stop_count > tenacity:\n",
    "            print(f\"Early stop after epoch {ep}.\")\n",
    "            break\n",
    "        # print(f\"Epoch {epoch:3>} loss: {running_loss}\")\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Epoch {ep:3>} train loss: {running_loss}\")\n",
    "        print(f\"Epoch {ep:3>} val loss  : {val_loss}\")\n",
    "        print(f\"Early stop count = {early_stop_count}\")\n",
    "print(\"Finished training!\")\n",
    "with torch.no_grad():\n",
    "    Y_hat = []\n",
    "    for i in range(6):  # X_test.shape[1]\n",
    "        if i == 0:\n",
    "            X = X_train[:, i + 1 :]\n",
    "        else:\n",
    "            X = np.concatenate(\n",
    "                (X_train[:, (i + 1) :], X_test[:, :i]), axis=1\n",
    "            )\n",
    "\n",
    "        sample = torch.from_numpy(X).to(device)\n",
    "        out = model(sample).cpu().detach().numpy().flatten()\n",
    "        Y_hat.append(out)\n",
    "\n",
    "forecast = np.stack(Y_hat, axis=1)\n",
    "# calculate mase (mae since we have already scaled)\n",
    "error = np.mean(np.abs(forecast - X_test))\n",
    "print(f\"MASE : {error}\")\n",
    "error_axis = np.mean(np.mean(np.abs(forecast - X_test), axis=1))\n",
    "print(f\"MASE axis : {error_axis}\")\n",
    "\n",
    "forecast = np.stack(Y_hat, axis=1)\n",
    "# calculate mase (mae since we have already scaled)\n",
    "error = np.mean(np.abs(forecast[-23000:] - X_test[-23000:]))\n",
    "print(f\"MASE yearly: {error}\")\n",
    "error_axis = np.mean(np.mean(np.abs(forecast[-23000:] - X_test[-23000:]), axis=1))\n",
    "print(f\"MASE axis yearly : {error_axis}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(6):  # X_test.shape[1]\n",
    "        sample = torch.from_numpy(X_train[:, -12:])\n",
    "        out = model(sample).cpu().detach().numpy()\n",
    "        X_train = np.hstack((X_train, out))\n",
    "\n",
    "forecast = X_train[:, -6:]\n",
    "error_axis = np.mean(np.mean(np.abs(forecast[-23000:] - X_test[-23000:]), axis=1))\n",
    "print(f\"MASE recursive yearly : {error_axis}\")"
   ]
  },
  {
   "source": [
    "# Current train script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteroMansoHyndmanSimpleDS(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset on the form of Monetero-Manso and Hyndman. Last 12:1 observations of each ts\n",
    "    from M4 as samples. Last obs as label. Scaled by MASE.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        arr:np.array,\n",
    "        memory:int, \n",
    "        ) -> None:\n",
    "        super(MonteroMansoHyndmanSimpleDS, self).__init__()\n",
    "        self.X = torch.from_numpy(arr)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :-1], self.X[idx, -1], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of learnable parameters: 3617\n",
      "Epoch 10 train loss: 63.489465232711986\n",
      "Epoch 10 val loss  : 35.28297568333003\n",
      "Early stop count = 1\n",
      "Epoch 20 train loss: 56.99212048817758\n",
      "Epoch 20 val loss  : 32.19493199900279\n",
      "Early stop count = 3\n",
      "Epoch 30 train loss: 56.02792458158725\n",
      "Epoch 30 val loss  : 31.078743880439536\n",
      "Early stop count = 0\n",
      "Epoch 40 train loss: 56.30400643171353\n",
      "Epoch 40 val loss  : 31.438770832556678\n",
      "Early stop count = 10\n",
      "Epoch 50 train loss: 55.57690514028805\n",
      "Epoch 50 val loss  : 31.168845647299236\n",
      "Early stop count = 8\n",
      "Epoch 60 train loss: 54.18120820517044\n",
      "Epoch 60 val loss  : 29.4082656900572\n",
      "Early stop count = 0\n",
      "Epoch 70 train loss: 51.396107737226885\n",
      "Epoch 70 val loss  : 30.765574373104354\n",
      "Early stop count = 1\n",
      "Epoch 80 train loss: 52.644673858308714\n",
      "Epoch 80 val loss  : 30.04044787959961\n",
      "Early stop count = 11\n",
      "Epoch 90 train loss: 50.729309496860914\n",
      "Epoch 90 val loss  : 29.40432281989407\n",
      "Early stop count = 4\n",
      "Epoch 100 train loss: 52.357727263124396\n",
      "Epoch 100 val loss  : 28.821769430792735\n",
      "Early stop count = 0\n",
      "Epoch 110 train loss: 50.32124923321692\n",
      "Epoch 110 val loss  : 29.60447470428918\n",
      "Early stop count = 10\n",
      "Epoch 120 train loss: 50.40957039338356\n",
      "Epoch 120 val loss  : 29.7415109180522\n",
      "Early stop count = 20\n",
      "Epoch 130 train loss: 51.74562463896876\n",
      "Epoch 130 val loss  : 29.40406103166938\n",
      "Early stop count = 4\n",
      "Epoch 140 train loss: 51.103071257273065\n",
      "Epoch 140 val loss  : 29.19521627748832\n",
      "Early stop count = 14\n",
      "Epoch 150 train loss: 51.15155600699844\n",
      "Epoch 150 val loss  : 29.361529441616632\n",
      "Early stop count = 24\n",
      "Early stop after epoch 152.\n",
      "Finished training!\n",
      "MASE : 0.8476490730176752\n",
      "MASE axis : 0.8476490730176751\n",
      "MASE yearly : 1.4673579763839038\n",
      "MASE axis yearly : 1.4673579763839035\n",
      "recursive forecasting Yearly\n",
      "MASE Yearly recursive: 3.5775094860584673\n",
      "MASE Yearly axis recursive: 3.577509486058468\n",
      "(23000, 12)\n",
      "(23000, 6)\n",
      "forecasting Yearly\n",
      "MASE Yearly one-step: 1.4673579763839038\n",
      "MASE Yearly_axis one-step: 1.4673579763839035\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(\"../M4_GLOBAL_train.npy\")\n",
    "X_test = np.load(\"../M4_GLOBAL_test.npy\")\n",
    "\n",
    "train = X_train[: int(X_train.shape[0] * 0.8), :]\n",
    "val = X_train[int(X_train.shape[0] * 0.8) :, :]\n",
    "\n",
    "\n",
    "#model = MLP(mem=12, n_hidden=32).double()\n",
    "model = MLP2(in_features=12, out_features=1, num_layers=5, n_hidden=32).double()\n",
    "print(f\"Number of learnable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#ds = MonteroMansoHyndmanSimpleDS(arr=train,memory=12,)\n",
    "#ds_val = MonteroMansoHyndmanSimpleDS(arr=val, memory=12,)\n",
    "ds = MonteroMansoHyndmanSimpleDS(arr=train, memory=12)\n",
    "ds_val = MonteroMansoHyndmanSimpleDS(arr=val, memory=12)\n",
    "\n",
    "trainloader = DataLoader(dataset=ds, batch_size=1024, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(dataset=ds_val, batch_size=1024, shuffle=True, num_workers=0)\n",
    "\n",
    "num_epochs = 1000\n",
    "tenacity = 25\n",
    "early_stop_count = 0\n",
    "low_loss = np.inf\n",
    "np.random.seed(1729)\n",
    "torch.manual_seed(1729)\n",
    "for ep in range(1, num_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.flatten(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data[0], data[1]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.flatten(), labels)\n",
    "            val_loss += loss.item()\n",
    "        if val_loss < low_loss:\n",
    "            early_stop_count = 0\n",
    "            low_loss = val_loss\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        if early_stop_count > tenacity:\n",
    "            print(f\"Early stop after epoch {ep}.\")\n",
    "            break\n",
    "        # print(f\"Epoch {epoch:3>} loss: {running_loss}\")\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Epoch {ep:3>} train loss: {running_loss}\")\n",
    "        print(f\"Epoch {ep:3>} val loss  : {val_loss}\")\n",
    "        print(f\"Early stop count = {early_stop_count}\")\n",
    "print(\"Finished training!\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y_hat = []\n",
    "    for i in range(6):  # X_test.shape[1]\n",
    "        if i == 0:\n",
    "            X = X_train[:, i + 1 :]\n",
    "        else:\n",
    "            X = np.concatenate((X_train[:, (i + 1) :], X_test[:, :i]), axis=1)\n",
    "        sample = torch.from_numpy(X)\n",
    "        out = model(sample).cpu().detach().numpy().flatten()\n",
    "        Y_hat.append(out)\n",
    "\n",
    "forecast = np.stack(Y_hat, axis=1)\n",
    "# calculate mase (mae since we have already scaled)\n",
    "error = np.mean(np.abs(forecast - X_test))\n",
    "error_axis = np.mean(np.mean(np.abs(forecast - X_test), axis=1))\n",
    "error_yearly = np.mean(np.abs(forecast[-23000:] - X_test[-23000:]))\n",
    "error_yearly_axis = np.mean(np.mean(np.abs(forecast[-23000:] - X_test[-23000:]), axis=1))\n",
    "\n",
    "print(f\"MASE : {error}\")\n",
    "print(f\"MASE axis : {error_axis}\")\n",
    "print(f\"MASE yearly : {error_yearly}\")\n",
    "print(f\"MASE axis yearly : {error_yearly_axis}\")\n",
    "\n",
    "# Testing yearly data recursive\n",
    "df_yearly_train = pd.read_csv(\"../GPTime/data/raw/M4/M4train/Yearly-train.csv\", index_col=0)\n",
    "df_yearly_test = pd.read_csv(\"../GPTime/data/raw/M4/M4test/Yearly-test.csv\", index_col=0)\n",
    "\n",
    "scale = (df_yearly_train.diff(periods=1, axis=1).abs().mean(axis=1).reset_index(drop=True))\n",
    "\n",
    "X_train_yearly = df_yearly_train.div(scale.values, axis=0).to_numpy()\n",
    "X_test_yearly = df_yearly_test.div(scale.values, axis=0).to_numpy()\n",
    "\n",
    "# X_train_yearly = df_yearly_train.to_numpy()\n",
    "# X_test_yearly = df_yearly_test.to_numpy()\n",
    "ts_train = []\n",
    "ts_test = []\n",
    "for i in range(X_train_yearly.shape[0]):\n",
    "    s_train = X_train_yearly[i][~np.isnan(X_train_yearly[i])]\n",
    "    s_test = X_test_yearly[i][~np.isnan(X_test_yearly[i])]\n",
    "    ts_train.append(s_train[-12:])  # shortest in the train set\n",
    "    ts_test.append(s_test[:6])  # shortest in the test set\n",
    "\n",
    "df_train_out = pd.DataFrame(ts_train)\n",
    "df_test_out = pd.DataFrame(ts_test)\n",
    "\n",
    "X_train = np.array(ts_train)\n",
    "X_test = np.array(ts_test)\n",
    "\n",
    "print(\"recursive forecasting Yearly\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(6):  # X_test.shape[1]\n",
    "        sample = torch.from_numpy(X_train[:, -12:])\n",
    "        out = model(sample).cpu().detach().numpy()\n",
    "        X_train = np.hstack((X_train, out))\n",
    "\n",
    "forecast = X_train[:, -6:]\n",
    "\n",
    "error = np.mean(np.abs(forecast - X_test))\n",
    "error_axis = np.mean(np.mean(np.abs(forecast - X_test), axis=1))\n",
    "print(f\"MASE Yearly recursive: {error}\")\n",
    "print(f\"MASE Yearly axis recursive: {error_axis}\")\n",
    "\n",
    "X_train = np.array(ts_train)\n",
    "X_test = np.array(ts_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(\"forecasting Yearly\")\n",
    "with torch.no_grad():\n",
    "    Y_hat = []\n",
    "    for i in range(6):  # X_test.shape[1]\n",
    "        if i == 0:\n",
    "            X = X_train[:, i:]\n",
    "        else:\n",
    "            X = np.concatenate((X_train[:, i:], X_test[:, :i]), axis=1)\n",
    "        sample = torch.from_numpy(X)\n",
    "        out = model(sample).cpu().detach().numpy().flatten()\n",
    "        Y_hat.append(out)\n",
    "\n",
    "forecast = np.stack(Y_hat, axis=1)\n",
    "\n",
    "error = np.mean(np.abs(forecast - X_test))\n",
    "print(f\"MASE Yearly one-step: {error}\")\n",
    "error_axis = np.mean(np.mean(np.abs(forecast - X_test), axis=1))\n",
    "print(f\"MASE Yearly_axis one-step: {error_axis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}