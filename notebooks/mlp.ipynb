{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599234732224",
   "display_name": "Python 3.7.6 64-bit ('env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(1729)\n",
    "torch.manual_seed(1729)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"GLOBAL\" : True,\n",
    "    \"YEARLY\" : False,\n",
    "    \"QUARTERLY\" : False,\n",
    "    \"MONTHLY\" : False,\n",
    "    \"WEEKLY\" : False,\n",
    "    \"DAILY\" : False,\n",
    "    \"HOURLY\" : False,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"MASE_SCALE\" : True,\n",
    "    \"MASE_SCALE_SEASONALITY\" : True,\n",
    "    \"SCALE_SET\" : \"FULL\" # FULL or SUB. Either the all the data or just the last 12 obs.\n",
    "}\n",
    "\n",
    "# Periods used in MASE scaling\n",
    "periods = {\n",
    "    \"YEARLY\"    : 1, \n",
    "    \"QUARTERLY\" : 4, \n",
    "    \"MONTHLY\"   : 12, \n",
    "    \"WEEKLY\"    : 52, \n",
    "    \"DAILY\"     : 7, \n",
    "    \"HOURLY\"    : 24\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating datasets for experiment GLOBAL.\n\n\nDone. Created datasets for ['GLOBAL'].\n\nSizes of the datasets: \nGLOBAL    : (100000,  13), (100000,  6)\n\n\n"
    }
   ],
   "source": [
    "all_train_files = glob.glob(\"..\\\\data\\\\M4train\\\\*\")\n",
    "all_test_files = glob.glob(\"..\\\\data\\\\M4test\\\\*\")\n",
    "datasets = preprocess.preprocess_dataset(experiments=experiments, config=config, periods=periods, all_test_files=all_test_files, all_train_files=all_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M4Dataset(Dataset):\n",
    "    \"\"\" Dataset for M4 AR(P) models \"\"\"\n",
    "    def __init__(self, arr):\n",
    "        self.X = torch.from_numpy(arr[:,:-1])\n",
    "        self.Y = torch.from_numpy(arr[:,-1])\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" N layer dense MLP \"\"\"\n",
    "    def __init__(self, memory, H=32):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = nn.Linear(memory, 1)\n",
    "        #self.l2 = nn.Linear(H, H)\n",
    "        #self.l3 = nn.Linear(H, H)\n",
    "        #self.l4 = nn.Linear(H, H)\n",
    "        #self.l5 = nn.Linear(H, 1)\n",
    "        self.init_weights()\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.l1(x))\n",
    "        #x = F.relu(self.l2(x))\n",
    "        #x = F.relu(self.l3(x))\n",
    "        #x = F.relu(self.l4(x))\n",
    "        #x = F.relu(self.l5(x))\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.l1.weight, std=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "GLOBAL\nEpoch 1 loss: 1500.2011872704886\nEpoch 1 loss: 270.075060242223\nEpoch 2 loss: 1393.3945276929944\nEpoch 2 loss: 249.09172916341535\nEpoch 3 loss: 1359.5647369165347\nEpoch 3 loss: 238.53038210023408\nEpoch 4 loss: 1342.8696911617972\nEpoch 4 loss: 232.21471101370355\nEpoch 5 loss: 1303.2067252887002\nEpoch 5 loss: 243.1909735210323\nEpoch 6 loss: 1292.3952779456047\nEpoch 6 loss: 224.44589766966297\nEpoch 7 loss: 1287.0534064376554\nEpoch 7 loss: 220.2492115476961\nEpoch 8 loss: 1289.6332733631477\nEpoch 8 loss: 219.04286162870258\nEpoch 9 loss: 1281.4007607568071\n1\nEpoch 9 loss: 240.2249775281866\nEpoch 10 loss: 1289.2218842115426\n1\nEpoch 10 loss: 225.6916653363802\nEpoch 11 loss: 1287.0443639444986\nEpoch 11 loss: 216.26381262834286\nEpoch 12 loss: 1294.158823536955\nEpoch 12 loss: 216.18160836873207\nEpoch 13 loss: 1284.607796048978\n1\nEpoch 13 loss: 254.51570638396421\nEpoch 14 loss: 1280.2143146515168\n1\nEpoch 14 loss: 244.94261040285556\nEpoch 15 loss: 1280.293171306835\n1\nEpoch 15 loss: 219.75279733881735\nEpoch 16 loss: 1280.4981955128344\nEpoch 16 loss: 215.9449700588809\nEpoch 17 loss: 1287.045985639434\n1\nEpoch 17 loss: 250.29741951249923\nEpoch 18 loss: 1285.7951601523382\n1\nEpoch 18 loss: 231.81121932839662\nEpoch 19 loss: 1280.5197263888836\n1\nEpoch 19 loss: 265.88022249975876\nEpoch 20 loss: 1287.694934511036\n1\nEpoch 20 loss: 253.29201641173614\nEpoch 21 loss: 1296.3169539589587\n1\nEpoch 21 loss: 242.3261939133343\nEpoch 22 loss: 1280.6725875937248\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-14383271fd3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\eriko\\OneDrive\\Documents\\projects\\GPTime-Series\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\eriko\\OneDrive\\Documents\\projects\\GPTime-Series\\env\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\eriko\\OneDrive\\Documents\\projects\\GPTime-Series\\env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2615\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2616\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2617\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2618\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feedforward_models = {}\n",
    "max_epochs = 10000\n",
    "batch_size = 1024\n",
    "tenacity = 7\n",
    "for d in datasets.keys():\n",
    "    # make datasetsx    \n",
    "    X_train = copy.deepcopy(datasets[d][0])\n",
    "    #np.random.shuffle(X_train)\n",
    "    split = int(X_train.shape[0]*0.85)\n",
    "    ds_train = M4Dataset(X_train[:split,:])\n",
    "    ds_val = M4Dataset(X_train[split:,:])\n",
    "\n",
    "    trainloader = DataLoader(ds_train, batch_size=1024, shuffle=True, num_workers=0)\n",
    "    valloader = DataLoader(ds_val, batch_size=1024, shuffle=True, num_workers=0)\n",
    "\n",
    "    # make model\n",
    "    net = MLP(X_train.shape[1]-1, H=32).double()\n",
    "\n",
    "    # train model\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.05)\n",
    "    print(d)\n",
    "    #print(f\"Start Training {d}.\")\n",
    "\n",
    "    val_losses = list(np.ones(10)*np.inf) # initialize for early stop\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        #if epoch%10==0:\n",
    "            #print(f\"Epoch {epoch:3>} loss: {running_loss}\")\n",
    "        print(f\"Epoch {epoch:3>} loss: {running_loss}\")\n",
    "\n",
    "        # Early stop\n",
    "        val_loss = 0\n",
    "        tenacity_count = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Early stop\n",
    "        if epoch > tenacity + 1:\n",
    "            if val_loss < min(val_losses[-tenacity:]):\n",
    "                tenacity_count = 0\n",
    "            elif epoch > tenacity:\n",
    "                tenacity_count += 1\n",
    "                print(tenacity_count)\n",
    "        val_losses.append(val_loss)\n",
    "        if tenacity_count >= tenacity:\n",
    "            break\n",
    "        print(f\"Epoch {epoch:3>} loss: {val_loss}\")\n",
    "\n",
    "    #print(f\"Finished Training {d}.\")\n",
    "    feedforward_models[d] = copy.deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'mase' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-97667223443a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mmase_feedforward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MASE_LINEAR\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MASE_FEEDFORWARD\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmase_feedforward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mase' is not defined"
     ]
    }
   ],
   "source": [
    "forecasts_feedforward = {}\n",
    "mase_feedforward = {}\n",
    "\n",
    "for model in feedforward_models.keys():\n",
    "    print(model)\n",
    "    net = feedforward_models[model]\n",
    "    net.eval()\n",
    "    # forecast\n",
    "    X_train = copy.deepcopy(datasets[model][0])\n",
    "    X_test = copy.deepcopy(datasets[model][1])\n",
    "    Y_hat = []\n",
    "    for i in range(X_test.shape[1]):\n",
    "        if i == 0:\n",
    "            X = X_train[:,i+1:]\n",
    "        else:\n",
    "            X = np.concatenate((X_train[:,(i+1):], X_test[:,:i]), axis=1)\n",
    "        assert X.shape[1] == len(mod.params)\n",
    "        X_tensor = torch.from_numpy(X)\n",
    "        pred = net(X_tensor).detach().numpy().flatten()\n",
    "        #print(pred.shape)\n",
    "        assert pred.shape == (X_test.shape[0],)\n",
    "        Y_hat.append(pred)\n",
    "\n",
    "    forecasts_feedforward[model] = np.stack(Y_hat, axis=1)\n",
    "    #print(forecasts_feedforward[model].shape)\n",
    "\n",
    "    # calculate mase (mae since we have already scaled)\n",
    "    error = np.mean(np.abs(forecasts_feedforward[model] - X_test))\n",
    "    mase_feedforward[model] = error\n",
    "\n",
    "df = pd.DataFrame({\"Model\": [e for e in experiments.keys() if experiments[e]], \"MASE_LINEAR\": [m for m in mase.values()], \"MASE_FEEDFORWARD\": [m for m in mase_feedforward.values()]})\n",
    "print(df.head(10))\n",
    "\n",
    "#display in nice table"
   ]
  }
 ]
}