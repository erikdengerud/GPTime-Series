# Model name for saving etc
name: test_name
# Learning rate Scheduler
lr_scheduler: #plateau #"multiplicative" # "plateau", "cosine", "cosine_warm"
patience: 20
# Stochastic Weight Averaging
swa: False

# Train Loop
max_epochs: 200
early_stop_tenacity: 50
log_freq: 5
clip_gradients: True
tensorboard_log_dir: GPTime/models/runs/
model_save_path:

# Dataset
more_data: False
proportion: 1
val: False
h_val: False
v_val: True
dataset_module: "GPTime.model.data"
dataset_name: "TSDataset"
dataset_params:
  lookback: 4
  dataset_paths:
    #FRED: GPTime/data/processed/FRED3/
    #FRED_small: GPTime/data/processed/FRED_small
    M4: GPTime/data/processed/M4
    #M4: /work/erikde/data/processed/M4/
    #FRED: /work/erikde/data/processed/FRED/
  frequencies:  
    Y: False
    Q: False
    M: False
    W: False
    D: True
    H: False
    #O: False
  min_lengths:
    Y: 9
    Q: 12
    M: 27
    W: 13
    D: 14
    H: 48
    O: 20 
  cutoff_date:

scaling:
  periods:
    Y: 1
    Q: 4
    M: 12
    W: 1
    D: 1
    H: 24
    O: 1

# Dataloader
train_set_size: 0.75 # 0-1
num_tests_per_ts: 1
scale: False
dataloader_module: "torch.utils.data"
dataloader_name: "DataLoader"
dataloader_params:
  batch_size: 1024
  shuffle: True
  num_workers: 4

# Change model here
seasonal_init: False
model_module: "GPTime.networks.mlp"
model_name: "MLP"
model_params_mlp:
  in_features: 30
  out_features: 1
  num_layers: 10
  n_hidden: 1024
  bias: True
  residual: None
  res_block_size: 4
  skip_connections: False
  dropout: 0.2
  encode_frequencies: True

# Optimizer
optimizer_module: "torch.optim"
optimizer_name: "Adam"
optimizer_params:
  lr: 0.001
  betas: [0.9, 0.999]
  eps: 0.00000001
  weight_decay: 0
  amsgrad: False

# Loss
criterion_module: GPTime.utils.loss
criterion_name: smape_loss
criterion_params:
  test: None
